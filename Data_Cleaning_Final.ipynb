{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0d5feab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import Normalizer\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cea3df3b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>URBRRL</th>\n",
       "      <th>RATCAT_A</th>\n",
       "      <th>INCGRP_A</th>\n",
       "      <th>INCTCFLG_A</th>\n",
       "      <th>FAMINCTC_A</th>\n",
       "      <th>IMPINCFLG_A</th>\n",
       "      <th>PPSU</th>\n",
       "      <th>PSTRAT</th>\n",
       "      <th>HISPALLP_A</th>\n",
       "      <th>RACEALLP_A</th>\n",
       "      <th>...</th>\n",
       "      <th>PROXYREL_A</th>\n",
       "      <th>PROXY_A</th>\n",
       "      <th>AVAIL_A</th>\n",
       "      <th>HHSTAT_A</th>\n",
       "      <th>INTV_QRT</th>\n",
       "      <th>RECTYPE</th>\n",
       "      <th>WTFA_A</th>\n",
       "      <th>WTIA_A</th>\n",
       "      <th>HHX</th>\n",
       "      <th>POVRATTC_A</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>60000</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>122</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>13177.008</td>\n",
       "      <td>7601.336</td>\n",
       "      <td>H048109</td>\n",
       "      <td>2.96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>50000</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>122</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>6140.552</td>\n",
       "      <td>3344.434</td>\n",
       "      <td>H027044</td>\n",
       "      <td>2.97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>12</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>65000</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>122</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>9191.061</td>\n",
       "      <td>6949.498</td>\n",
       "      <td>H058855</td>\n",
       "      <td>4.28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>14</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>120000</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>122</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>7900.035</td>\n",
       "      <td>6446.327</td>\n",
       "      <td>H031993</td>\n",
       "      <td>7.13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>30000</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>115</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>10875.772</td>\n",
       "      <td>8646.586</td>\n",
       "      <td>H007122</td>\n",
       "      <td>1.13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31992</th>\n",
       "      <td>4</td>\n",
       "      <td>14</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>116204</td>\n",
       "      <td>0</td>\n",
       "      <td>100</td>\n",
       "      <td>114</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "      <td>2679.016</td>\n",
       "      <td>1965.416</td>\n",
       "      <td>H046022</td>\n",
       "      <td>7.65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31993</th>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>68000</td>\n",
       "      <td>0</td>\n",
       "      <td>100</td>\n",
       "      <td>114</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "      <td>6726.495</td>\n",
       "      <td>3920.208</td>\n",
       "      <td>H046232</td>\n",
       "      <td>2.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31994</th>\n",
       "      <td>4</td>\n",
       "      <td>13</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>60000</td>\n",
       "      <td>0</td>\n",
       "      <td>100</td>\n",
       "      <td>114</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "      <td>1246.306</td>\n",
       "      <td>864.743</td>\n",
       "      <td>H043765</td>\n",
       "      <td>4.98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31995</th>\n",
       "      <td>4</td>\n",
       "      <td>14</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>101000</td>\n",
       "      <td>0</td>\n",
       "      <td>100</td>\n",
       "      <td>114</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "      <td>3427.198</td>\n",
       "      <td>2595.209</td>\n",
       "      <td>H017695</td>\n",
       "      <td>6.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31996</th>\n",
       "      <td>4</td>\n",
       "      <td>13</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>79000</td>\n",
       "      <td>0</td>\n",
       "      <td>100</td>\n",
       "      <td>114</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "      <td>3453.332</td>\n",
       "      <td>1891.706</td>\n",
       "      <td>H026306</td>\n",
       "      <td>4.69</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>31997 rows × 534 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       URBRRL  RATCAT_A  INCGRP_A  INCTCFLG_A  FAMINCTC_A  IMPINCFLG_A  PPSU  \\\n",
       "0           4         9         3           0       60000            2     2   \n",
       "1           4         9         3           0       50000            0     2   \n",
       "2           4        12         3           0       65000            1     2   \n",
       "3           4        14         5           0      120000            0     2   \n",
       "4           1         4         1           0       30000            0     2   \n",
       "...       ...       ...       ...         ...         ...          ...   ...   \n",
       "31992       4        14         5           0      116204            0   100   \n",
       "31993       4         8         3           0       68000            0   100   \n",
       "31994       4        13         3           0       60000            0   100   \n",
       "31995       4        14         5           0      101000            0   100   \n",
       "31996       4        13         4           0       79000            0   100   \n",
       "\n",
       "       PSTRAT  HISPALLP_A  RACEALLP_A  ...  PROXYREL_A  PROXY_A  AVAIL_A  \\\n",
       "0         122           3           2  ...         NaN      NaN        1   \n",
       "1         122           2           1  ...         NaN      NaN        1   \n",
       "2         122           2           1  ...         NaN      NaN        1   \n",
       "3         122           1           8  ...         NaN      NaN        1   \n",
       "4         115           2           1  ...         NaN      NaN        1   \n",
       "...       ...         ...         ...  ...         ...      ...      ...   \n",
       "31992     114           2           1  ...         NaN      NaN        1   \n",
       "31993     114           2           1  ...         NaN      NaN        1   \n",
       "31994     114           2           1  ...         NaN      NaN        1   \n",
       "31995     114           1           1  ...         NaN      NaN        1   \n",
       "31996     114           2           1  ...         NaN      NaN        1   \n",
       "\n",
       "       HHSTAT_A  INTV_QRT  RECTYPE     WTFA_A    WTIA_A      HHX  POVRATTC_A  \n",
       "0             1         1       10  13177.008  7601.336  H048109        2.96  \n",
       "1             1         1       10   6140.552  3344.434  H027044        2.97  \n",
       "2             1         1       10   9191.061  6949.498  H058855        4.28  \n",
       "3             1         1       10   7900.035  6446.327  H031993        7.13  \n",
       "4             1         1       10  10875.772  8646.586  H007122        1.13  \n",
       "...         ...       ...      ...        ...       ...      ...         ...  \n",
       "31992         1         4       10   2679.016  1965.416  H046022        7.65  \n",
       "31993         1         4       10   6726.495  3920.208  H046232        2.02  \n",
       "31994         1         4       10   1246.306   864.743  H043765        4.98  \n",
       "31995         1         4       10   3427.198  2595.209  H017695        6.00  \n",
       "31996         1         4       10   3453.332  1891.706  H026306        4.69  \n",
       "\n",
       "[31997 rows x 534 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#LOAD DATA\n",
    "path = 'adult19.csv'\n",
    "interviewData = pd.read_csv (path)\n",
    "interviewData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7ae08f4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>COLUMN_INDEX</th>\n",
       "      <th>COLUMN_NAME</th>\n",
       "      <th>DESCRIPTION</th>\n",
       "      <th>DATA_TYPE</th>\n",
       "      <th>CATEGORICAL_TYPE</th>\n",
       "      <th>NANs</th>\n",
       "      <th>REPLACE_WITH</th>\n",
       "      <th>DROP_NAN_CODE_1</th>\n",
       "      <th>DROP_NAN_CODE_2</th>\n",
       "      <th>DROP_NAN_CODE_3</th>\n",
       "      <th>...</th>\n",
       "      <th>Code_4_Meaning</th>\n",
       "      <th>Code_5</th>\n",
       "      <th>Code_5_Meaning</th>\n",
       "      <th>Code_6</th>\n",
       "      <th>Code_6_Meaning</th>\n",
       "      <th>Code_7</th>\n",
       "      <th>Code_7_Meaning</th>\n",
       "      <th>Code_8</th>\n",
       "      <th>Code_8_Meaning</th>\n",
       "      <th>NOTES</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>402.0</td>\n",
       "      <td>POVRATTC_A</td>\n",
       "      <td>SA family poverty ratio</td>\n",
       "      <td>numerical</td>\n",
       "      <td>NaN</td>\n",
       "      <td>drop_nans</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>403.0</td>\n",
       "      <td>HHX</td>\n",
       "      <td>Randomly assigned household number unique to a...</td>\n",
       "      <td>recordkeeping - ID</td>\n",
       "      <td>NaN</td>\n",
       "      <td>drop_nans</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>404.0</td>\n",
       "      <td>WTIA_A</td>\n",
       "      <td>Weight - annual pre-post stratification calibr...</td>\n",
       "      <td>recordkeeping</td>\n",
       "      <td>NaN</td>\n",
       "      <td>drop_col</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>405.0</td>\n",
       "      <td>WTFA_A</td>\n",
       "      <td>Weight - Final Annual</td>\n",
       "      <td>recordkeeping</td>\n",
       "      <td>NaN</td>\n",
       "      <td>drop_col</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>406.0</td>\n",
       "      <td>RECTYPE</td>\n",
       "      <td>Record type</td>\n",
       "      <td>recordkeeping</td>\n",
       "      <td>NaN</td>\n",
       "      <td>drop_col</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>Sample Child Income</td>\n",
       "      <td>50.0</td>\n",
       "      <td>Paradata</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>529</th>\n",
       "      <td>264.0</td>\n",
       "      <td>PHQ82_A</td>\n",
       "      <td>How often feeling down, past 2 weeks</td>\n",
       "      <td>categorical</td>\n",
       "      <td>ordinal</td>\n",
       "      <td>drop_col</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>Nearly every day</td>\n",
       "      <td>7.0</td>\n",
       "      <td>Refused</td>\n",
       "      <td>8.0</td>\n",
       "      <td>Not Ascertained</td>\n",
       "      <td>9.0</td>\n",
       "      <td>Don't Know</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>PHQ questions</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>530</th>\n",
       "      <td>265.0</td>\n",
       "      <td>PHQ81_A</td>\n",
       "      <td>How often little interest in things, past 2 weeks</td>\n",
       "      <td>categorical</td>\n",
       "      <td>ordinal</td>\n",
       "      <td>drop_col</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>Nearly every day</td>\n",
       "      <td>7.0</td>\n",
       "      <td>Refused</td>\n",
       "      <td>8.0</td>\n",
       "      <td>Not Ascertained</td>\n",
       "      <td>9.0</td>\n",
       "      <td>Don't Know</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>PHQ questions</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>531</th>\n",
       "      <td>266.0</td>\n",
       "      <td>MHTHND_A</td>\n",
       "      <td>Needed counseling/therapy but did not get it d...</td>\n",
       "      <td>categorical</td>\n",
       "      <td>nominal</td>\n",
       "      <td>drop_nans</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>Nearly every day</td>\n",
       "      <td>7.0</td>\n",
       "      <td>Refused</td>\n",
       "      <td>8.0</td>\n",
       "      <td>Not Ascertained</td>\n",
       "      <td>9.0</td>\n",
       "      <td>Don't Know</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>532</th>\n",
       "      <td>267.0</td>\n",
       "      <td>MHTHDLY_A</td>\n",
       "      <td>Delayed counseling/therapy due to cost, past 12m</td>\n",
       "      <td>categorical</td>\n",
       "      <td>nominal</td>\n",
       "      <td>drop_nans</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>Nearly every day</td>\n",
       "      <td>7.0</td>\n",
       "      <td>Refused</td>\n",
       "      <td>8.0</td>\n",
       "      <td>Not Ascertained</td>\n",
       "      <td>9.0</td>\n",
       "      <td>Don't Know</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>533</th>\n",
       "      <td>NaN</td>\n",
       "      <td>MHTPYNOW_A</td>\n",
       "      <td>Currently receiving counseling/therapy from me...</td>\n",
       "      <td>categorical</td>\n",
       "      <td>nominal</td>\n",
       "      <td>replace</td>\n",
       "      <td>10.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>Not Ascertained</td>\n",
       "      <td>9.0</td>\n",
       "      <td>Don't Know</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>534 rows × 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     COLUMN_INDEX COLUMN_NAME  \\\n",
       "0           402.0  POVRATTC_A   \n",
       "1           403.0         HHX   \n",
       "2           404.0      WTIA_A   \n",
       "3           405.0      WTFA_A   \n",
       "4           406.0     RECTYPE   \n",
       "..            ...         ...   \n",
       "529         264.0     PHQ82_A   \n",
       "530         265.0     PHQ81_A   \n",
       "531         266.0    MHTHND_A   \n",
       "532         267.0   MHTHDLY_A   \n",
       "533           NaN  MHTPYNOW_A   \n",
       "\n",
       "                                           DESCRIPTION           DATA_TYPE  \\\n",
       "0                              SA family poverty ratio           numerical   \n",
       "1    Randomly assigned household number unique to a...  recordkeeping - ID   \n",
       "2    Weight - annual pre-post stratification calibr...       recordkeeping   \n",
       "3                                Weight - Final Annual       recordkeeping   \n",
       "4                                          Record type       recordkeeping   \n",
       "..                                                 ...                 ...   \n",
       "529               How often feeling down, past 2 weeks        categorical    \n",
       "530  How often little interest in things, past 2 weeks        categorical    \n",
       "531  Needed counseling/therapy but did not get it d...        categorical    \n",
       "532   Delayed counseling/therapy due to cost, past 12m        categorical    \n",
       "533  Currently receiving counseling/therapy from me...        categorical    \n",
       "\n",
       "    CATEGORICAL_TYPE       NANs  REPLACE_WITH  DROP_NAN_CODE_1  \\\n",
       "0                NaN  drop_nans           NaN              NaN   \n",
       "1                NaN  drop_nans           NaN              NaN   \n",
       "2                NaN   drop_col           NaN              NaN   \n",
       "3                NaN   drop_col           NaN              NaN   \n",
       "4                NaN   drop_col           NaN              NaN   \n",
       "..               ...        ...           ...              ...   \n",
       "529          ordinal   drop_col           NaN              NaN   \n",
       "530          ordinal   drop_col           NaN              NaN   \n",
       "531          nominal  drop_nans           NaN              NaN   \n",
       "532          nominal  drop_nans           NaN              NaN   \n",
       "533          nominal    replace          10.0              NaN   \n",
       "\n",
       "     DROP_NAN_CODE_2  DROP_NAN_CODE_3  ...       Code_4_Meaning Code_5  \\\n",
       "0                NaN              NaN  ...                  NaN    NaN   \n",
       "1                NaN              NaN  ...                  NaN    NaN   \n",
       "2                NaN              NaN  ...                  NaN    NaN   \n",
       "3                NaN              NaN  ...                  NaN    NaN   \n",
       "4                NaN              NaN  ...  Sample Child Income   50.0   \n",
       "..               ...              ...  ...                  ...    ...   \n",
       "529              NaN              NaN  ...     Nearly every day    7.0   \n",
       "530              NaN              NaN  ...     Nearly every day    7.0   \n",
       "531              NaN              NaN  ...     Nearly every day    7.0   \n",
       "532              NaN              NaN  ...     Nearly every day    7.0   \n",
       "533              NaN              NaN  ...      Not Ascertained    9.0   \n",
       "\n",
       "    Code_5_Meaning Code_6   Code_6_Meaning Code_7  Code_7_Meaning Code_8  \\\n",
       "0              NaN    NaN              NaN    NaN             NaN    NaN   \n",
       "1              NaN    NaN              NaN    NaN             NaN    NaN   \n",
       "2              NaN    NaN              NaN    NaN             NaN    NaN   \n",
       "3              NaN    NaN              NaN    NaN             NaN    NaN   \n",
       "4         Paradata    NaN              NaN    NaN             NaN    NaN   \n",
       "..             ...    ...              ...    ...             ...    ...   \n",
       "529        Refused    8.0  Not Ascertained    9.0      Don't Know    NaN   \n",
       "530        Refused    8.0  Not Ascertained    9.0      Don't Know    NaN   \n",
       "531        Refused    8.0  Not Ascertained    9.0      Don't Know    NaN   \n",
       "532        Refused    8.0  Not Ascertained    9.0      Don't Know    NaN   \n",
       "533     Don't Know    NaN              NaN    NaN             NaN    NaN   \n",
       "\n",
       "     Code_8_Meaning          NOTES  \n",
       "0               NaN            NaN  \n",
       "1               NaN            NaN  \n",
       "2               NaN            NaN  \n",
       "3               NaN            NaN  \n",
       "4               NaN            NaN  \n",
       "..              ...            ...  \n",
       "529             NaN  PHQ questions  \n",
       "530             NaN  PHQ questions  \n",
       "531             NaN            NaN  \n",
       "532             NaN            NaN  \n",
       "533             NaN            NaN  \n",
       "\n",
       "[534 rows x 29 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#LOAD CSV WITH DATA CLEANING INSTRUCTIONS\n",
    "path = 'VariableNanDetails.csv'\n",
    "data_cleaning_inst = pd.read_csv (path)\n",
    "data_cleaning_inst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7641f2a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      COLUMN_NAME                                                                       DESCRIPTION\n",
      "2          WTIA_A                               Weight - annual pre-post stratification calibration\n",
      "3          WTFA_A                                                             Weight - Final Annual\n",
      "4         RECTYPE                                                                       Record type\n",
      "6        HHSTAT_A                                              Indicates person is the Sample Adult\n",
      "137   IMPINCFLG_A                                         Imputed SA family income imputation flag \n",
      "138          PPSU                                Pseudo-PSU for public-use file variance estimation\n",
      "139        PSTRAT                     \\r\\nPseudo-stratum for public-use file variance estimation \\r\n",
      "165  HHRESPSA_FLG  Sample Adult is the household respondent or the proxy who lives in the household\n",
      "194       OGFLG_A                                                Other government reassignment flag\n",
      "195       OPFLG_A                                                 State-sponsored reassignment flag\n",
      "196       CHFLG_A                                                            CHIP reassignment flag\n",
      "197       MAFLG_A                                                        Medicaid reassignment flag\n",
      "203       PRFLG_A                                                         Private reassignment flag\n",
      "204   PLEXCHPR1_A                  Exchange company coding, NCHS, reassigned from public to private\n",
      "205     PRPREM1_A                                 Premium on plan reassigned from public to private\n",
      "206     PXCHNG1_A                  Marketplace or state exchange, reassigned from public to private\n",
      "219    PLEXCHOG_A                          Exchange company coding, NCHS (other government program)\n",
      "220    PLEXCHOP_A                              Exchange company coding, NCHS (state-sponsored plan)\n",
      "429       SRVY_YR                                      Year of the National Health Interview Survey\n",
      "(31997, 515)\n"
     ]
    }
   ],
   "source": [
    "#DROP RECORDKEEPING COLUMNS\n",
    "\n",
    "#make working copy of data\n",
    "cleanData = interviewData.copy()\n",
    "#make list of columns to drop \n",
    "cols = data_cleaning_inst.COLUMN_NAME[(data_cleaning_inst.DATA_TYPE == 'recordkeeping') &\n",
    "                                      (data_cleaning_inst.NANs == 'drop_col')].tolist()\n",
    "# drop irrelevant columns\n",
    "cleanData = cleanData.drop(columns = cols)\n",
    "\n",
    "#list columns that were dropped\n",
    "print(data_cleaning_inst[['COLUMN_NAME', 'DESCRIPTION']][data_cleaning_inst.COLUMN_NAME.isin(cols)].to_string())\n",
    "#print clean shape\n",
    "print(cleanData.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "42e76e05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    COLUMN_NAME                                              DESCRIPTION\n",
      "523     PHQ88_A  How often moving or speaking slow or fast, past 2 weeks\n",
      "524     PHQ87_A            How often trouble concentrating, past 2 weeks\n",
      "525     PHQ86_A           How often feeling bad about self, past 2 weeks\n",
      "526     PHQ85_A        How often undereating or overeating, past 2 weeks\n",
      "527     PHQ84_A                    How often feeling tired, past 2 weeks\n",
      "528     PHQ83_A            How often trouble with sleeping, past 2 weeks\n",
      "529     PHQ82_A                     How often feeling down, past 2 weeks\n",
      "530     PHQ81_A        How often little interest in things, past 2 weeks\n",
      "(31997, 507)\n"
     ]
    }
   ],
   "source": [
    "#DROP PHQ COLUMNS\n",
    "\n",
    "#make list of columns to drop \n",
    "cols = data_cleaning_inst.COLUMN_NAME[(data_cleaning_inst.COLUMN_NAME.str.contains('PHQ', case=False)) &\n",
    "    (data_cleaning_inst.NANs == 'drop_col')].tolist()\n",
    "\n",
    "# drop irrelevant columns\n",
    "cleanData = cleanData.drop(columns = cols)\n",
    "\n",
    "#list columns that were dropped\n",
    "print(data_cleaning_inst[['COLUMN_NAME', 'DESCRIPTION']][data_cleaning_inst.COLUMN_NAME.isin(cols)].to_string())\n",
    "#print clean shape\n",
    "print(cleanData.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2ba24c0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "POVRATTC_A        0\n",
      "HHX               0\n",
      "INTV_QRT          0\n",
      "AVAIL_A           0\n",
      "HYPEV_A           0\n",
      "CHLEV_A           0\n",
      "CHDEV_A           0\n",
      "ASEV_A            0\n",
      "CANEV_A           0\n",
      "PREDIB_A          0\n",
      "DIBEV_A           0\n",
      "COPDEV_A          0\n",
      "ARTHEV_A          0\n",
      "DEMENEV_A         0\n",
      "ANXEV_A           0\n",
      "DEPEV_A           0\n",
      "WEARGLSS_A        0\n",
      "HEARAID_A         0\n",
      "SOCWRKLIM_A       0\n",
      "URBRRL            0\n",
      "RATCAT_A          0\n",
      "INCGRP_A          0\n",
      "FAMINCTC_A        0\n",
      "HISPALLP_A        0\n",
      "RACEALLP_A        0\n",
      "DISAB3_A          0\n",
      "CITZNSTP_A        0\n",
      "LEGMSTAT_A        0\n",
      "MARSTAT_A         0\n",
      "SMKECIGST_A       0\n",
      "SMKCIGST_A        0\n",
      "OTHGOV_A          0\n",
      "OTHPUB_A          0\n",
      "IHS_A             0\n",
      "MILITARY_A        0\n",
      "CHIP_A            0\n",
      "MEDICAID_A        0\n",
      "MEDICARE_A        0\n",
      "PRIVATE_A         0\n",
      "EDUC_A            0\n",
      "MAXEDUC_A        95\n",
      "PARSTAT_A         0\n",
      "SAPARENTSC_A      0\n",
      "MLTFAMFLG_A       0\n",
      "OVER65FLG_A       0\n",
      "HISDETP_A         0\n",
      "HISP_A            0\n",
      "REGION            0\n",
      "MHTHRPY_A         0\n",
      "DEPMED_A          0\n",
      "DEPFREQ_A         0\n",
      "ANXMED_A          0\n",
      "ANXFREQ_A         0\n",
      "HOMEHC12M_A       0\n",
      "THERA12M_A        0\n",
      "EYEEX12M_A        0\n",
      "SHTFLU12M_A       0\n",
      "MEDNG12M_A        0\n",
      "MEDDL12M_A        0\n",
      "HOSPONGT_A        0\n",
      "USUALPL_A         0\n",
      "DENNG12M_A        0\n",
      "DENDL12M_A        0\n",
      "DENPREV_A         0\n",
      "PAYWORRY_A        0\n",
      "PAYBLL12M_A       0\n",
      "SEX_A             0\n",
      "HOUYRSLIV_A       0\n",
      "FDSBALANCE_A      0\n",
      "FDSLAST_A         0\n",
      "FDSRUNOUT_A       0\n",
      "INCOTHR_A       920\n",
      "INCRETIRE_A     920\n",
      "INCWELF_A       920\n",
      "INCSSISSDI_A    920\n",
      "INCSSRR_A       920\n",
      "INCINTER_A        0\n",
      "INCWRKO_A         0\n",
      "SCHCURENR_A       0\n",
      "NATUSBORN_A       0\n",
      "MARITAL_A         0\n",
      "ORIENT_A          0\n",
      "CIGAREV_A         0\n",
      "ECIGEV_A          0\n",
      "SMKEV_A           0\n",
      "PAIFRQ3M_A      157\n",
      "MHTHND_A          0\n",
      "MHTHDLY_A         0\n",
      "(30992, 507)\n"
     ]
    }
   ],
   "source": [
    "#DROP ROWS WITH NANS\n",
    "\n",
    "#make list of columns to look for nans in \n",
    "cols = data_cleaning_inst.COLUMN_NAME[data_cleaning_inst.NANs.isin(['drop_nans', 'drop_nan_and_nan_codes'])]\n",
    "#count nans per col\n",
    "nans_drop_nans = cleanData[cols].isnull().sum(axis = 0)\n",
    "    \n",
    "# #drop rows with nans in cols\n",
    "cleanData = cleanData.dropna(subset=cols)\n",
    "\n",
    "#show nans per col\n",
    "print(nans_drop_nans.to_string())\n",
    "#print clean shape\n",
    "print(cleanData.shape)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8cfc0604",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all relevant cols have missing codes: True\n",
      "missing codes remaining:  0.0\n",
      "(27778, 507)\n"
     ]
    }
   ],
   "source": [
    "#CATEGORICAL ORDINAL DATA WITH NO 'NOT APPLICABLE' RESPONDENTS\n",
    "#remove 'missing data codes' rows\n",
    "#eg - not ascertained, refused, don't know\n",
    "\n",
    "#check all columns with 'drop_nan_codes' instructions have missing data codes \n",
    "#data columns that need missing data codes remove\n",
    "miss_code_col_A = data_cleaning_inst.COLUMN_NAME[data_cleaning_inst.NANs.isin(\n",
    "    ['drop_nan_codes', 'drop_nan_and_nan_codes'])]\n",
    "\n",
    "#instruction columns with missing data codes\n",
    "miss_code_cols = ['DROP_NAN_CODE_1', 'DROP_NAN_CODE_2', 'DROP_NAN_CODE_3', 'DROP_NAN_CODE_4']\n",
    "#select cols with missing data codes\n",
    "miss_code_col_B = data_cleaning_inst.dropna(subset=miss_code_cols, how='all').COLUMN_NAME\n",
    "\n",
    "#check they they are the same\n",
    "miss_code_col_A = miss_code_col_A.sort_values()\n",
    "miss_code_col_B = miss_code_col_B.sort_values()\n",
    "\n",
    "#they are the same\n",
    "print('all relevant cols have missing codes:', miss_code_col_A.equals(miss_code_col_B))\n",
    "\n",
    "#make a copy (for ease in troubleshooting)\n",
    "cleanData_copy = cleanData.copy()\n",
    "\n",
    "#delete any rows with missing data codes in these rows\n",
    "miss_code_col = miss_code_col_A.to_list()\n",
    "for c in miss_code_col:\n",
    "    miss_data_codes = data_cleaning_inst[miss_code_cols][data_cleaning_inst.COLUMN_NAME == c].dropna(axis=1).to_numpy().flatten()\n",
    "    cleanData_copy = cleanData_copy[cleanData_copy[c].isin(miss_data_codes) == False]   \n",
    "\n",
    "#check no missing codes remain     \n",
    "miss_codes_remaining = []\n",
    "for c in miss_code_col:\n",
    "    unique_vals = np.sort(cleanData_copy[c].unique())\n",
    "    miss_data_codes = np.sort(data_cleaning_inst[miss_code_cols][data_cleaning_inst.COLUMN_NAME == c].dropna(axis=1).to_numpy().flatten())\n",
    "    intersection = np.intersect1d(unique_vals, miss_data_codes)\n",
    "    miss_codes_remaining.append(intersection)   \n",
    "\n",
    "print('missing codes remaining: ', np.sum(np.asarray(miss_codes_remaining)))\n",
    "\n",
    "#save cleanData_copy as copy\n",
    "cleanData = cleanData_copy.copy()\n",
    "#look at shape \n",
    "print(cleanData.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7d538f6c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all relevant cols have missing codes: True\n",
      "Number NaNs left:  0\n",
      "(27778, 507)\n"
     ]
    }
   ],
   "source": [
    "#CATEGORICAL DATA: ADD NOT APPLICABLE CODES\n",
    "\n",
    "#check all columns with 'replace' instructions have replace_with values\n",
    "#data columns that need NaNs replaced\n",
    "replaceNaNs_cols_A = data_cleaning_inst.COLUMN_NAME[data_cleaning_inst.NANs=='replace']\n",
    "\n",
    "#select cols with replace-with values\n",
    "replaceNaNs_cols_B = data_cleaning_inst.COLUMN_NAME[data_cleaning_inst.REPLACE_WITH.notna()]\n",
    "\n",
    "#check they they are the same\n",
    "replaceNaNs_cols_A = replaceNaNs_cols_A.sort_values()\n",
    "replaceNaNs_cols_B = replaceNaNs_cols_B.sort_values()\n",
    "\n",
    "#they are the same\n",
    "print('all relevant cols have missing codes:', replaceNaNs_cols_A.equals(replaceNaNs_cols_B))\n",
    "\n",
    "#make a copy (for ease in troubleshooting)\n",
    "cleanData_copy = cleanData.copy()\n",
    "\n",
    "#replace NaNs with appropriate code\n",
    "replaceNaNs_cols = replaceNaNs_cols_A.to_list()\n",
    "for c in replaceNaNs_cols:\n",
    "    replace_code = data_cleaning_inst.REPLACE_WITH[data_cleaning_inst.COLUMN_NAME == c].to_numpy()[0]\n",
    "    cleanData_copy[c] = cleanData_copy[c].fillna(replace_code)\n",
    "    \n",
    "#CHECK THERE ARE NO NANs LEFT\n",
    "print('Number NaNs left: ', cleanData_copy.isnull().sum().sum())\n",
    "\n",
    "#save cleanData_copy as copy\n",
    "cleanData = cleanData_copy.copy()\n",
    "#look at shape \n",
    "print(cleanData.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f715c73a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(27778, 5295)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#ONE-HOT ENCODE CATEGORICAL DATA\n",
    "#make list of columns to one-hot encode\n",
    "cols = data_cleaning_inst.COLUMN_NAME[data_cleaning_inst.ENCODING == 'one_hot'].tolist()\n",
    "#isolate data to encode as one-hot\n",
    "clean_onehot_data = cleanData.loc[:, cols]\n",
    "#create object\n",
    "enc = OneHotEncoder()\n",
    "#fit encoder\n",
    "enc.fit(clean_onehot_data)\n",
    "#transform data\n",
    "clean_onehot_data = enc.transform(clean_onehot_data).toarray()\n",
    "#get name of new columns\n",
    "onehot_features = enc.get_feature_names_out(cols)\n",
    "#join with previous data \n",
    "onehot_df = pd.DataFrame(clean_onehot_data, columns = onehot_features, index = cleanData.index)\n",
    "cleanData = cleanData.drop(columns = cols)\n",
    "cleanData = pd.concat([cleanData, onehot_df], axis=1)\n",
    "cleanData.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c30edd87",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ISOLATE TARGET\n",
    "\n",
    "#select one-hot encoded columns rep target\n",
    "target_cols = cleanData.columns[cleanData.columns.str.contains('PHQCAT_A').tolist()]\n",
    "TARGET = cleanData[target_cols]\n",
    "\n",
    "#remove target from features\n",
    "FEATURES = cleanData.drop(columns = target_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5f99c97d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SPLIT DATA INTO TRAINING AND TESTING\n",
    "x_train, x_test, y_train, y_test = train_test_split(FEATURES, TARGET, test_size=0.20, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a3945f0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#NORMALIZATION\n",
    "\n",
    "#get column names of numerical data\n",
    "cols = data_cleaning_inst.COLUMN_NAME[data_cleaning_inst.DATA_TYPE == 'numerical']\n",
    "#isolate categorical data to join later\n",
    "x_train_cat = x_train.drop(columns = cols)\n",
    "x_test_cat = x_test.drop(columns = cols)\n",
    "\n",
    "#isolate data to normalize\n",
    "x_train_num = x_train[cols]\n",
    "x_test_num = x_test[cols]\n",
    "\n",
    "#create object and fit to training data\n",
    "normalizer = Normalizer().fit(x_train_num)\n",
    "\n",
    "#training data\n",
    "#normalize \n",
    "x_train_norm = normalizer.transform (x_train_num)\n",
    "#turn to dataframe\n",
    "x_train_norm = pd.DataFrame(x_train_norm, index = x_train_num.index, columns = x_train_num.columns)\n",
    "#join with cat data\n",
    "x_train_final = pd.concat([x_train_cat, x_train_norm], axis=1)\n",
    "\n",
    "#test data\n",
    "#normalize test data using same transformer\n",
    "x_test_norm = normalizer.transform (x_test_num)\n",
    "#turn to dataframe\n",
    "x_test_norm = pd.DataFrame(x_test_norm, index = x_test_num.index, columns = x_test_num.columns)\n",
    "#join with cat data\n",
    "x_test_final = pd.concat([x_test_cat, x_test_norm], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "34e70c8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#STORE DATA\n",
    "\n",
    "#make directory \n",
    "dataDir = 'CleanDataFinal'\n",
    "if not os.path.exists(dataDir):\n",
    "    os.mkdir(dataDir)\n",
    "\n",
    "#store training data\n",
    "filepath = os.path.join(dataDir, 'trainFeaturesFinal.csv')\n",
    "x_train_final.to_csv(filepath) \n",
    "filepath = os.path.join(dataDir, 'trainTargetFinal.csv')\n",
    "y_train.to_csv(filepath) \n",
    "\n",
    "#store testing data\n",
    "filepath = os.path.join(dataDir, 'testFeaturesFinal.csv')\n",
    "x_test_final.to_csv(filepath) \n",
    "filepath = os.path.join(dataDir, 'testTargetFinal.csv')\n",
    "y_test.to_csv(filepath) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3eb1d4c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainX shape: (22222, 5292)\n",
      "trainY shape: (22222, 5)\n",
      "testX shape: (5556, 5292)\n",
      "testY shape: (5556, 5)\n"
     ]
    }
   ],
   "source": [
    "#TEST TO MAKE SURE DATA CAN BE READ\n",
    "dataDir = 'CleanDataFinal'\n",
    "\n",
    "filepath = os.path.join(dataDir, 'trainFeaturesFinal.csv')\n",
    "trainX = pd.read_csv (filepath)\n",
    "print('trainX shape:', trainX.shape)\n",
    "\n",
    "filepath = os.path.join(dataDir, 'trainTargetFinal.csv')\n",
    "trainY = pd.read_csv (filepath)\n",
    "print('trainY shape:', trainY.shape)\n",
    "\n",
    "filepath = os.path.join(dataDir, 'testFeaturesFinal.csv')\n",
    "testX = pd.read_csv (filepath)\n",
    "print('testX shape:', testX.shape)\n",
    "\n",
    "filepath = os.path.join(dataDir, 'testTargetFinal.csv')\n",
    "testY = pd.read_csv (filepath)\n",
    "print('testY shape:', testY.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c0674aa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MachineLearningUVM",
   "language": "python",
   "name": "machinelearninguvm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
