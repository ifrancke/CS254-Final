{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "154d9fe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import Normalizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "98586ae7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5556, 505)\n",
      "(22222, 505)\n"
     ]
    }
   ],
   "source": [
    "#LOAD NONHOT DATA\n",
    "\n",
    "#training data\n",
    "dataDir = 'AnalysisData'\n",
    "dataSubDir = 'CleanDataNonhot'\n",
    "filepath = os.path.join(dataDir, dataSubDir, 'testFeaturesNonHot.csv')\n",
    "test_x_pd_nonhot = pd.read_csv (filepath, index_col = 0)\n",
    "test_x_nonhot = test_x_pd_nonhot.to_numpy()\n",
    "original_shape = test_x_pd_nonhot.shape\n",
    "print(original_shape)\n",
    "\n",
    "#testing data\n",
    "dataDir = 'AnalysisData'\n",
    "dataSubDir = 'CleanDataNonhot'\n",
    "filepath = os.path.join(dataDir, dataSubDir, 'trainFeaturesNonHot.csv')\n",
    "train_x_pd_nonhot = pd.read_csv (filepath, index_col = 0)\n",
    "train_x_nonhot = train_x_pd_nonhot.to_numpy()\n",
    "print(train_x_nonhot.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "851a0434",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#LOAD CSV WITH DATA CLEANING INSTRUCTIONS\n",
    "path = 'VariableNanDetails.csv'\n",
    "data_cleaning_inst = pd.read_csv (path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3341503c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5556, 5290)\n"
     ]
    }
   ],
   "source": [
    "#LOAD HOTENCONDED TESTING DATA TO MAKE SURE SIZE MATCHES\n",
    "dataDir = 'AnalysisData'\n",
    "dataSubDir = 'CleanDataFinal'\n",
    "\n",
    "#LOAD TESTING DATA\n",
    "#features\n",
    "path = os.path.join(dataDir, dataSubDir, 'testFeaturesFinal.csv')\n",
    "test_x_pd = pd.read_csv (path, index_col = 'HHX')\n",
    "test_x = test_x_pd.to_numpy()\n",
    "shape_wanted = test_x.shape\n",
    "print(shape_wanted)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd31d228",
   "metadata": {},
   "source": [
    "## REMOVE DATA RANDOMLY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "28892417",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FUNCTION TO REMOVE DATA RANDOMLY\n",
    "def remove_random_data(complete_data, data_percent):\n",
    "    '''replaces random entries (single features from a row) with NaN.\n",
    "    complete_data: array containing full dataset\n",
    "    data_percent: integer indicating what percent of data to remove\n",
    "    returns: array containing dataset with missing data (NaNs)'''\n",
    "    #make copy of array \n",
    "    full_data = np.copy(complete_data)\n",
    "    original_shape = full_data.shape\n",
    "    #flatten data\n",
    "    full_data = full_data.flatten()\n",
    "    #make array for incomplete data\n",
    "    inc_data = np.copy(full_data)\n",
    "\n",
    "    #calculate number of entries to replace\n",
    "    entries_to_remove = int(full_data.size*data_percent/100)\n",
    "\n",
    "    #choose random indeces of data to replace\n",
    "    i = np.random.choice(full_data.shape[0], entries_to_remove, replace=False)\n",
    "    #replace random data\n",
    "    for index in i:\n",
    "        inc_data[index] = np.nan\n",
    "    #reshape data back to original shape\n",
    "    inc_data = np.reshape(inc_data, original_shape)\n",
    "    return(inc_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6eeb081",
   "metadata": {},
   "source": [
    "## DATA IMPUTATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eca6f740",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5556, 505)\n",
      "(5556, 505)\n"
     ]
    }
   ],
   "source": [
    "#remove 15% of data\n",
    "data_percent = 15\n",
    "test_x_inc = remove_random_data(test_x_nonhot, data_percent)\n",
    "#turn to dataframe\n",
    "test_x_inc_pd = pd.DataFrame(test_x_inc, columns=test_x_pd_nonhot.columns, index=test_x_pd_nonhot.index)\n",
    "print(original_shape)\n",
    "print(test_x_inc_pd.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f47c8644",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5556, 4)\n",
      "(5556, 501)\n"
     ]
    }
   ],
   "source": [
    "#ISOLATE NUMERICAL AND CATEGORICAL FEATURES\n",
    "#numerical features\n",
    "col_num = data_cleaning_inst.COLUMN_NAME[\n",
    "    (data_cleaning_inst.DATA_TYPE=='numerical') & (data_cleaning_inst.NANs != 'drop_col')]\n",
    "data_num = test_x_inc_pd.loc[:, col_num]\n",
    "print(data_num.shape)\n",
    "#categorical features\n",
    "data_cat = test_x_inc_pd.drop(columns=col_num)\n",
    "print(data_cat.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8487675",
   "metadata": {},
   "source": [
    "## UNIVARIATE IMPUTATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "beba95a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5556, 4)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#NUMERICAL DATA\n",
    "#turn to array\n",
    "data_num_array = data_num.to_numpy()\n",
    "#create imputer object\n",
    "num_imp = SimpleImputer(missing_values=np.nan, strategy='median')\n",
    "#fit simple imputer\n",
    "num_imp.fit(data_num_array)\n",
    "#impute missing data\n",
    "data_num_imp = num_imp.transform(data_num_array)\n",
    "#turn to dataframe\n",
    "data_num_imp = pd.DataFrame(data_num_imp, columns=data_num.columns, index=test_x_pd_nonhot.index)\n",
    "data_num_imp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "359a8db3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5556, 501)\n",
      "(5556, 501)\n"
     ]
    }
   ],
   "source": [
    "#CATEGORICAL DATA\n",
    "#turn to array\n",
    "data_cat_array = data_cat.to_numpy()\n",
    "#create imputer object\n",
    "cat_imp = SimpleImputer(missing_values=np.nan, strategy='most_frequent')\n",
    "#fit simple imputer\n",
    "cat_imp.fit(data_cat_array)\n",
    "#impute missing data\n",
    "data_cat_imp = cat_imp.transform(data_cat_array)\n",
    "#turn to dataframe\n",
    "data_cat_imp = pd.DataFrame(data_cat_imp, columns=data_cat.columns, index=test_x_pd_nonhot.index)\n",
    "print(data_cat.shape)\n",
    "print(data_cat_imp.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "153cad93",
   "metadata": {},
   "source": [
    "## NORMALIZE NUMERICAL DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "82eb60b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5556, 4)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#USE TRAINING DATA TO TRAIN ENCODER, THEN NORMALIZE NUM TEST DATA \n",
    "\n",
    "#isolate numerical training data\n",
    "col_num = data_cleaning_inst.COLUMN_NAME[\n",
    "    (data_cleaning_inst.DATA_TYPE=='numerical') & (data_cleaning_inst.NANs != 'drop_col')]\n",
    "num_train = train_x_pd_nonhot.loc[:, col_num]\n",
    "\n",
    "#create object and fit to training data\n",
    "normalizer = Normalizer().fit(num_train)\n",
    "\n",
    "#test data\n",
    "num_norm_imp = normalizer.transform (data_num_imp)\n",
    "#turn to dataframe\n",
    "num_norm_imp_pd = pd.DataFrame(num_norm_imp, columns = data_num_imp.columns, index = data_num_imp.index)\n",
    "num_norm_imp_pd.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73162630",
   "metadata": {},
   "source": [
    "## ONE-HOT ENCODING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ec429bc0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(27778, 505)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#USE ALL FEATURES TO TRAIN ENCODER\n",
    "x_all = pd.concat([test_x_pd_nonhot, train_x_pd_nonhot], axis=0)\n",
    "x_all.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "af94d6bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5556, 5246)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#ONE-HOT ENCODE CATEGORICAL DATA\n",
    "#make list of columns to one-hot encode\n",
    "cols = data_cleaning_inst.COLUMN_NAME[(data_cleaning_inst.ENCODING == 'one_hot') & \n",
    "                                      (data_cleaning_inst.COLUMN_NAME != 'PHQCAT_A')].tolist()\n",
    "\n",
    "#isolate data to encode as one-hot\n",
    "x_all_onehot = x_all.loc[:, cols]\n",
    "#create object\n",
    "enc = OneHotEncoder()\n",
    "#fit encoder\n",
    "enc.fit(x_all_onehot)\n",
    "\n",
    "#transform data\n",
    "#isolate data to onehot encode\n",
    "cat_to_onehot = data_cat_imp.loc[:, cols]\n",
    "cat_onehot = enc.transform(cat_to_onehot).toarray()\n",
    "onehot_features = enc.get_feature_names_out(cols)\n",
    "#turn to df\n",
    "onehot_df = pd.DataFrame(cat_onehot, columns = onehot_features, index=cat_to_onehot.index)\n",
    "onehot_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0869af9",
   "metadata": {},
   "source": [
    "## JOIN DATA AND SAVE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cf5f302c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5556, 5290)\n",
      "(5556, 5290)\n",
      "0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AVAIL_A_1</th>\n",
       "      <th>AVAIL_A_2</th>\n",
       "      <th>AVAIL_A_3</th>\n",
       "      <th>AVAIL_A_8</th>\n",
       "      <th>PROXY_A_1.0</th>\n",
       "      <th>PROXY_A_10.0</th>\n",
       "      <th>PROXYREL_A_1.0</th>\n",
       "      <th>PROXYREL_A_2.0</th>\n",
       "      <th>PROXYREL_A_3.0</th>\n",
       "      <th>PROXYREL_A_4.0</th>\n",
       "      <th>...</th>\n",
       "      <th>COGMEMDFF_A</th>\n",
       "      <th>COMDIFF_A</th>\n",
       "      <th>DIFF_A</th>\n",
       "      <th>HEARINGDF_A</th>\n",
       "      <th>VISIONDF_A</th>\n",
       "      <th>PHSTAT_A</th>\n",
       "      <th>POVRATTC_A</th>\n",
       "      <th>INTV_QRT</th>\n",
       "      <th>FAMINCTC_A</th>\n",
       "      <th>AGEP_A</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HHX</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>H038561</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.000050</td>\n",
       "      <td>0.000020</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>H043980</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.000045</td>\n",
       "      <td>0.000018</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>H052102</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000032</td>\n",
       "      <td>0.000033</td>\n",
       "      <td>0.999999</td>\n",
       "      <td>0.001017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>H015803</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.000045</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>H000267</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.000051</td>\n",
       "      <td>0.000021</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>H035143</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.000064</td>\n",
       "      <td>0.000036</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>H035351</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000050</td>\n",
       "      <td>0.000015</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>H019908</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000023</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>H038324</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000039</td>\n",
       "      <td>0.000041</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>H021041</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.000126</td>\n",
       "      <td>0.000036</td>\n",
       "      <td>0.999999</td>\n",
       "      <td>0.001071</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5556 rows Ã— 5290 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         AVAIL_A_1  AVAIL_A_2  AVAIL_A_3  AVAIL_A_8  PROXY_A_1.0  \\\n",
       "HHX                                                                \n",
       "H038561        1.0        0.0        0.0        0.0          0.0   \n",
       "H043980        1.0        0.0        0.0        0.0          0.0   \n",
       "H052102        1.0        0.0        0.0        0.0          0.0   \n",
       "H015803        1.0        0.0        0.0        0.0          0.0   \n",
       "H000267        0.0        0.0        1.0        0.0          1.0   \n",
       "...            ...        ...        ...        ...          ...   \n",
       "H035143        1.0        0.0        0.0        0.0          0.0   \n",
       "H035351        1.0        0.0        0.0        0.0          0.0   \n",
       "H019908        1.0        0.0        0.0        0.0          0.0   \n",
       "H038324        1.0        0.0        0.0        0.0          0.0   \n",
       "H021041        1.0        0.0        0.0        0.0          0.0   \n",
       "\n",
       "         PROXY_A_10.0  PROXYREL_A_1.0  PROXYREL_A_2.0  PROXYREL_A_3.0  \\\n",
       "HHX                                                                     \n",
       "H038561           1.0             0.0             0.0             0.0   \n",
       "H043980           1.0             0.0             0.0             0.0   \n",
       "H052102           1.0             0.0             0.0             0.0   \n",
       "H015803           1.0             0.0             0.0             0.0   \n",
       "H000267           0.0             1.0             0.0             0.0   \n",
       "...               ...             ...             ...             ...   \n",
       "H035143           1.0             0.0             0.0             0.0   \n",
       "H035351           1.0             0.0             0.0             0.0   \n",
       "H019908           1.0             0.0             0.0             0.0   \n",
       "H038324           1.0             0.0             0.0             0.0   \n",
       "H021041           1.0             0.0             0.0             0.0   \n",
       "\n",
       "         PROXYREL_A_4.0  ...  COGMEMDFF_A  COMDIFF_A  DIFF_A  HEARINGDF_A  \\\n",
       "HHX                      ...                                                \n",
       "H038561             0.0  ...          1.0        1.0     1.0          2.0   \n",
       "H043980             0.0  ...          1.0        1.0     1.0          1.0   \n",
       "H052102             0.0  ...          1.0        1.0     1.0          1.0   \n",
       "H015803             0.0  ...          1.0        1.0     1.0          1.0   \n",
       "H000267             0.0  ...          2.0        1.0     1.0          1.0   \n",
       "...                 ...  ...          ...        ...     ...          ...   \n",
       "H035143             0.0  ...          1.0        1.0     1.0          1.0   \n",
       "H035351             0.0  ...          1.0        1.0     1.0          1.0   \n",
       "H019908             0.0  ...          1.0        1.0     1.0          1.0   \n",
       "H038324             0.0  ...          1.0        1.0     1.0          1.0   \n",
       "H021041             0.0  ...          1.0        1.0     1.0          1.0   \n",
       "\n",
       "         VISIONDF_A  PHSTAT_A  POVRATTC_A  INTV_QRT  FAMINCTC_A    AGEP_A  \n",
       "HHX                                                                        \n",
       "H038561         1.0       2.0    0.000050  0.000020    1.000000  0.000135  \n",
       "H043980         1.0       2.0    0.000045  0.000018    1.000000  0.000314  \n",
       "H052102         2.0       1.0    0.000032  0.000033    0.999999  0.001017  \n",
       "H015803         1.0       2.0    0.000045  0.000014    1.000000  0.000318  \n",
       "H000267         1.0       3.0    0.000051  0.000021    1.000000  0.000885  \n",
       "...             ...       ...         ...       ...         ...       ...  \n",
       "H035143         1.0       2.0    0.000064  0.000036    1.000000  0.000782  \n",
       "H035351         1.0       1.0    0.000050  0.000015    1.000000  0.000265  \n",
       "H019908         1.0       1.0    0.000023  0.000013    1.000000  0.000348  \n",
       "H038324         1.0       1.0    0.000039  0.000041    1.000000  0.000449  \n",
       "H021041         1.0       2.0    0.000126  0.000036    0.999999  0.001071  \n",
       "\n",
       "[5556 rows x 5290 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#join with other data\n",
    "cat_nonhot = data_cat_imp.drop(columns = cols)\n",
    "x_train_imp = pd.concat([onehot_df, cat_nonhot, num_norm_imp_pd], axis=1)\n",
    "\n",
    "#a few checks\n",
    "print(x_train_imp.shape)\n",
    "print(shape_wanted)\n",
    "#print # NaNs\n",
    "print(x_train_imp.isnull().sum().sum())\n",
    "\n",
    "x_train_imp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "83146711",
   "metadata": {},
   "outputs": [],
   "source": [
    "#STORE DATA\n",
    "\n",
    "#make directory \n",
    "dataDir = 'ImputedData'\n",
    "if not os.path.exists(dataDir):\n",
    "    os.mkdir(dataDir)\n",
    "\n",
    "#store imputed testing data\n",
    "filepath = os.path.join(dataDir, 'SimpleImputedFeatures_05.csv')\n",
    "x_train_imp.to_csv(filepath) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f133e1c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:MachineLearningUVM] *",
   "language": "python",
   "name": "conda-env-MachineLearningUVM-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
